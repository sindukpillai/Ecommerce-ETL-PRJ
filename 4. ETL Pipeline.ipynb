{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "<center><h1> Project: ETL Pipeline </h1></center>\n",
    "\n",
    "----\n",
    "\n",
    "* **ETL Pipeline 1:** Update User Summary Table\n",
    "* **ETL Pipeline 2:** Update Transaction Summary Table\n",
    "* **ETL Pipeline 3:** Update Valid Refund Table\n",
    "\n",
    "----\n",
    "\n",
    "We have already setup the simulation environment for our ETL Project. So, new data is already coming to our MySQL tables.\n",
    "\n",
    "<br>\n",
    "\n",
    "In this notebook, we will create the required pipelines one by one and finally use the schedular library to schedule all three pipelines.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Importing the required libraries`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "import schedule\n",
    "import time\n",
    "import pandas as pd\n",
    "import mysql.connector as mysql\n",
    "from datetime import timedelta\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "***Define a function to create the connection with the MySQL database. `Configure the following cell as per your system settings.`***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_connection():\n",
    "    db = mysql.connect(\n",
    "    host = \"localhost\",\n",
    "    user = \"lakshay\", ## Enter your username here\n",
    "    password = \"ABC@123\", ## Enter your password here\n",
    "    database = \"website\", ## Enter your database name here\n",
    "    auth_plugin = \"mysql_native_password\",\n",
    ") \n",
    "   \n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "<center><h1> ETL Pipeline 1 </h1></center>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "![](images/pipeline-1.png)\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### `Define the Extraction Function`\n",
    "\n",
    "---\n",
    "\n",
    "* ***`extract_users_data()`***: It will extract the data from the `users` table within defined time range and return the dataframe.\n",
    "\n",
    "<br>\n",
    "\n",
    "* ***`Paramaters Required`***:\n",
    "    * `Database Connection String` is required so that it can connect to the database and query the data. \n",
    "    * `Start time` and `End time` is required so that we can modify the query.\n",
    "\n",
    "<br>\n",
    "\n",
    "* ***`Output`***: It will return the pandas dataframe of the extracted data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_users_data(db, start_time, end_time):\n",
    "    \n",
    "    # create database cursor\n",
    "    cursor = db.cursor()\n",
    "    \n",
    "    print(\"Extracting signup results between {} and {}\".format(str(start_time),str(end_time)))\n",
    "    \n",
    "    # command to extract the data of last 5 minutes.\n",
    "    command = \"SELECT * FROM users WHERE signup_time BETWEEN '{}' AND '{}'\".format(start_time, end_time)\n",
    "    \n",
    "    # execute the command and return the results.\n",
    "    cursor.execute(command)\n",
    "    data = cursor.fetchall()\n",
    "    \n",
    "    # return the dataframe\n",
    "    return pd.DataFrame.from_records(data, columns= ['user_id',\n",
    "                                                     'user_email',\n",
    "                                                     'user_name',\n",
    "                                                     'source',\n",
    "                                                     'is_prime',\n",
    "                                                     'signup_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Define the Trasformation Function`\n",
    "\n",
    "---\n",
    "\n",
    "* ***`transform_users_data()`***: It will use the data from the `extract_users_data()` of the last 5 minutes and do the following transformation using pandas.\n",
    "    * Replace the category `Not Available` with the `Organic` in the source column.\n",
    "    * Use the groupby function to calculate number of users in each category of source.\n",
    "    * Use the groupby function to calculate number of prime users in each category of source.\n",
    "    * Store all the results in a dictionary \n",
    "    * Add the start & end time in the dictionary and return it.\n",
    "    \n",
    "<br>    \n",
    "\n",
    "* ***`Paramaters Required`***:\n",
    "    * `df_user` user data dataframe of last 5 minutes. \n",
    "    * `Start time` and `End time` is required to update the output, so that we know the signup summary is between this particular time range.\n",
    "\n",
    "<br>\n",
    "\n",
    "* ***`Output`***: It will return the dictionary that will be used to import the data into the users summary table.\n",
    "\n",
    "---\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the tranform function\n",
    "def transform_user_data(df_user, start_time, end_time):\n",
    "    \n",
    "    print(\"Transforming User Data...\")\n",
    "    \n",
    "    # replace the \"Not Available\" with the \"Organic\"\n",
    "    df_user.source.replace(\"Not Available\", \"Organic\", inplace=True)\n",
    "    \n",
    "    # groupby to calculate the number of users in each category of source.\n",
    "    source_count = df_user.groupby(['source'])['user_id'].count()\n",
    "    \n",
    "    # groupby to calculate the number of prime users in each category of source.\n",
    "    prime_count  = df_user.groupby(['source'])['is_prime'].sum()\n",
    "    \n",
    "    # create dictionary of source count\n",
    "    source_count_dict = source_count.to_dict()\n",
    "    \n",
    "    # append prime count in the same dictionary\n",
    "    for key in prime_count.to_dict():\n",
    "        new_key_name = \"prime_from_\" + key\n",
    "        source_count_dict[new_key_name] = prime_count[key]\n",
    "    \n",
    "    # add start_time and end_time to the dictionary \n",
    "    source_count_dict['start_time'] = str(start_time)\n",
    "    source_count_dict['end_time'] = str(end_time)\n",
    "    \n",
    "    # return the final dictionary\n",
    "    return source_count_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Define the Loading Function`\n",
    "\n",
    "---\n",
    "\n",
    "* ***`load_users_summary()`***: It will use the results dictionary from the `transform_users_data` and load it into the `signup_summary_table`.\n",
    "\n",
    "    \n",
    "<br>    \n",
    "\n",
    "* ***`Paramaters Required`***:\n",
    "\n",
    "    * `result_dict` final results dictionary from the transform function. \n",
    "    * `db` database connection string to update the values in the table.\n",
    "\n",
    "<br>\n",
    "\n",
    "* ***`Output`***: It will not return anything.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_user_summary(result_dict, db):\n",
    "    print(\"Loading User Summary Table...\")\n",
    "    cursor = db.cursor()\n",
    "    \n",
    "    # command to insert the data into the signup summary table using result dict\n",
    "    command = \"INSERT INTO signup_summary({col}) values{val}\".format(col= ','.join(result_dict.keys()),\n",
    "                                                                     val= tuple(result_dict.values()))\n",
    "    cursor.execute(command)\n",
    "    db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "#### `Define the ETL Pipeline`\n",
    "\n",
    "<br>\n",
    "\n",
    "Now, we will define the pipeline function, we will take the database connection as the parameter and do the following steps.\n",
    "\n",
    " * Now, we will define the time for which we want to extract the data.\n",
    " * Extract the latest 5 minutes of users data.\n",
    " * Transform it to get the users signup summary.\n",
    " * Load the data into the signup_summary table.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def pipeline_to_update_user_summary(db_object):\n",
    "    \n",
    "    # get the current time and time before 5 minutes\n",
    "    current_time = datetime.datetime.now()\n",
    "    current_minus_5 = current_time - datetime.timedelta(minutes=5)\n",
    "    \n",
    "    print(\"========================================================================================\")\n",
    "    print(\"Starting ETL to update user summary!!\")\n",
    "    \n",
    "    ## EXTRACTION\n",
    "    latest_user_data = extract_users_data(db = db_object,\n",
    "                                          start_time = current_minus_5,\n",
    "                                          end_time=current_time)\n",
    "    \n",
    "    ## TRANSFORMATION\n",
    "    user_summary_data = transform_user_data(df_user= latest_user_data,\n",
    "                                            start_time=current_minus_5, \n",
    "                                            end_time=current_time)\n",
    "    \n",
    "    ## LOADING\n",
    "    load_user_summary(result_dict = user_summary_data,\n",
    "                      db = db_object)\n",
    "    \n",
    "    print(\"Successfully loaded the data into user summary table !!\")\n",
    "    print(\"========================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's execute the pipeline 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "<center><h1> ETL Pipeline 2 </h1></center>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "![](images/pipeline-2.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Define the extraction functions`\n",
    "\n",
    "---\n",
    "\n",
    "* ***`extract_products_data()`***: It will read the products data from the CSV file. \n",
    "\n",
    "<br>\n",
    "\n",
    "* ***`Paramaters Required`***: CSV file path is required. It is present in the `dataset` folder.\n",
    "\n",
    "<br>\n",
    "\n",
    "* ***`Output`***: It will return the pandas dataframe of the CSV file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to extract the product_data from the CSV file\n",
    "def extract_products_data(file_path):\n",
    "    return pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "* ***`extract_transaction_data()`***: It will extract the data from the `transaction` table within a defined time range and return the dataframe.\n",
    "\n",
    "<br>\n",
    "\n",
    "* ***`Paramaters Required`***:\n",
    "    * `Database Connection String` is required so that it can connect to the database and query the data. \n",
    "    * `Start time` and `End time` is required so that we can modify the query.\n",
    "\n",
    "<br>\n",
    "\n",
    "* ***`Output`***: It will return the pandas dataframe of the extracted data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_transaction_data(db, start_time, end_time):\n",
    "    \n",
    "    # create database cursor\n",
    "    cursor = db.cursor()\n",
    "    \n",
    "    print(\"Extracting transactions between {} and {}\".format(str(start_time),str(end_time)))\n",
    "    \n",
    "    # command to extract the data of last 5 minutes.\n",
    "    command = \"SELECT * FROM transaction WHERE transaction_time BETWEEN '{}' AND '{}'\".format(start_time, end_time)\n",
    "    \n",
    "    # execute the command and return the results.\n",
    "    cursor.execute(command)\n",
    "    data = cursor.fetchall()\n",
    "    \n",
    "    # return the dataframe\n",
    "    return pd.DataFrame.from_records(data, columns= ['transaction_id',\n",
    "                                                     'user_id',\n",
    "                                                     'product_id',\n",
    "                                                     'transaction_time',\n",
    "                                                     'price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Define the Trasformation Function`\n",
    "\n",
    "---\n",
    "\n",
    "* ***`transform_transaction_data()`***: It will use the data from the `extract_transform_data()` of the last 10 minutes and the output of `extract_products_data()` and do the following transformation using pandas.\n",
    "    * Do the left join on `transction_data` and the `products_data`.\n",
    "    * Split the product_name and create a new feature `brand`.\n",
    "    * Use groupby to calculate the brand-wise sales.\n",
    "    * Use groupby to calculate the category-wise sales. \n",
    "    * Create dictionary of the calculated results.\n",
    "    * Add start and end time to the dictionary.\n",
    "    \n",
    "<br>    \n",
    "\n",
    "* ***`Paramaters Required`***:\n",
    "    * `df_transaction` transaction data dataframe of last 10 minutes. \n",
    "    * `df_product` product data frame.\n",
    "    * `Start time` and `End time` is required to update the output, so that we know the signup summary is between this particulae time range.\n",
    "\n",
    "<br>\n",
    "\n",
    "* ***`Output`***: It will return the dictionary which will be used to import the data into transaction summary.\n",
    "\n",
    "---\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the tranform function\n",
    "def transform_transaction_data(df_transaction, df_product, start_time, end_time):\n",
    "    \n",
    "    print(\"Transforming Transaction Data...\")\n",
    "    \n",
    "    # merge the transaction and product dataframe.\n",
    "    merged_df = df_transaction.merge(df_product, how='left', on='product_id')\n",
    "    \n",
    "    # split the product_name to get the brand  \n",
    "    merged_df['brand'] = merged_df['product_name'].apply(lambda x: x.split()[0])\n",
    "    \n",
    "    # calculate the brand count\n",
    "    brand_count = merged_df.groupby(['brand'])['transaction_id'].count()\n",
    "    \n",
    "    # calculate the category count\n",
    "    category_count = merged_df.groupby(['product_category'])['transaction_id'].count()\n",
    "    \n",
    "    # calculate the brand wise sales\n",
    "    brand_wise_sales    = merged_df.groupby(['brand'])['price'].sum()\n",
    "    \n",
    "    # calculate the category wise sales\n",
    "    category_wise_sales = merged_df.groupby(['product_category'])['price'].sum()\n",
    "    \n",
    "    # create dictionary \n",
    "    brand_count_dict = brand_count.to_dict()\n",
    "    \n",
    "    # append brand_count to dictionary\n",
    "    for key in category_count.to_dict():\n",
    "        brand_count_dict[key] = category_count[key]\n",
    "    \n",
    "    # append brand wise sales to dictionary\n",
    "    for key in brand_wise_sales.to_dict():\n",
    "        new_key_name = \"sales_from_\" + key\n",
    "        brand_count_dict[new_key_name] = brand_wise_sales[key]\n",
    "    \n",
    "    # append category wise sales to dictionary\n",
    "    for key in category_wise_sales.to_dict():\n",
    "        new_key_name = \"sales_from_\" + key\n",
    "        brand_count_dict[new_key_name] = category_wise_sales[key]\n",
    "    \n",
    "    # append start and end time to the dictionary\n",
    "    brand_count_dict['start_time'] = str(start_time)\n",
    "    brand_count_dict['end_time'] = str(end_time)\n",
    "    \n",
    "    return brand_count_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Define the Loading Function`\n",
    "\n",
    "---\n",
    "\n",
    "* ***`load_transaction_summary()`***: It will use the results dictionary from the `transform_transaction_data` and load it into the `transaction_summary_table`.\n",
    "\n",
    "    \n",
    "<br>    \n",
    "\n",
    "* ***`Paramaters Required`***:\n",
    "    * `result_dict` final results dictionary from the transform function. \n",
    "    * `db` database connection string to update the values in the table.\n",
    "\n",
    "<br>\n",
    "\n",
    "* ***`Output`***: It will not return anything.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transaction_summary(result_dict, db):\n",
    "    print(\"Loading Transaction Summary Table...\")\n",
    "    cursor = db.cursor()\n",
    "    \n",
    "    # command to insert the data into the transaction summary using the \n",
    "    command = \"INSERT INTO transaction_summary({col}) values{val}\".format(col= ', '.join(result_dict.keys()),\n",
    "                                                                       val= tuple(result_dict.values()))\n",
    "    \n",
    "    # we need to make some changes to the command, because we have space in two columns\n",
    "    # \"Air Conditioner\" and \"sales_from_Air conditioner\"\n",
    "    # Here we will have to replace Air Conditioner with `Air Conditioner`\n",
    "    command = command.replace(' Air Conditioner', '`Air Conditioner`')\n",
    "    command = command.replace('sales_from_Air Conditioner', '`sales_from_Air Conditioner`')\n",
    "    \n",
    "    # execute the commad\n",
    "    cursor.execute(command)\n",
    "    db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "#### `Define the ETL Pipeline`\n",
    "\n",
    "<br>\n",
    "\n",
    "Now, we will define the pipeline function, we will take the database connection as the parameter and do the following steps.\n",
    "\n",
    " * we will define the time for which we want to extract the data.\n",
    " * Extract the latest 10 minutes of transactions data.\n",
    " * Transform it to get the transactions summary.\n",
    " * Load the data into the transaction_summary table.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_to_update_transaction_summary(db_object, products_data):\n",
    "    # get the current time and time before 10 minutes\n",
    "    current_time = datetime.datetime.now()\n",
    "    current_minus_10 = current_time - datetime.timedelta(minutes=10)\n",
    "    \n",
    "    print(\"========================================================================================\")\n",
    "    print(\"Starting ETL to update transaction summary!!\")\n",
    "    \n",
    "    ## EXTRACTION\n",
    "    latest_transaction_data = extract_transaction_data(db = db_object,\n",
    "                                                       start_time = current_minus_10,\n",
    "                                                       end_time = current_time)\n",
    "    \n",
    "    ## TRANSFORMATION\n",
    "    transaction_summary_data = transform_transaction_data(df_product = products_data,\n",
    "                                                          df_transaction = latest_transaction_data,\n",
    "                                                          start_time = current_minus_10,\n",
    "                                                          end_time = current_time)\n",
    "    ## LOADING\n",
    "    load_transaction_summary(result_dict = transaction_summary_data,\n",
    "                             db = db_object)\n",
    "    \n",
    "    print(\"Successfully loaded the data into transaction summary table !!\")\n",
    "    print(\"========================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's execute the pipeline 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "\n",
    "<center><h1> ETL Pipeline 3 </h1></center>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "![](images/pipeline-3.png)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "* ***`extract_refund_data()`***: It will extract the data from the `refund_detail` table within the defined time range and return the dataframe.\n",
    "\n",
    "* ***`Paramaters Required`***:\n",
    "    * `Database Connection String` is required so that it can connect to the database and query the data. \n",
    "    * `Start time` and `End time` is required so that we can modify the query.\n",
    "\n",
    "* ***`Output`***: It will return the pandas dataframe of the CSV file.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_refund_data(db, start_time, end_time):\n",
    "    \n",
    "    # create database cursor\n",
    "    cursor = db.cursor()\n",
    "    \n",
    "    print(\"Extracting refund between {} and {}\".format(str(start_time),str(end_time)))\n",
    "    \n",
    "    # command to extract the data of last 30 minutes.\n",
    "    command = \"SELECT * FROM refund_detail WHERE ticket_raise_time BETWEEN '{}' AND '{}'\".format(start_time, end_time)\n",
    "    \n",
    "    # execute the command and return the results.\n",
    "    cursor.execute(command)\n",
    "    data = cursor.fetchall()\n",
    "    \n",
    "    # return the dataframe\n",
    "    return pd.DataFrame.from_records(data, columns= ['ticket_id',\n",
    "                                                     'user_name',\n",
    "                                                     'transaction_id',\n",
    "                                                     'transaction_amount',\n",
    "                                                     'ticket_raise_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "* ***`extract_valid_refund_data()`***: It will extract only the `transaction_id` from the `valid_refund` table within defined time range and return the dataframe.\n",
    "\n",
    "* ***`Paramaters Required`***:\n",
    "    * `Database Connection String` is required so that it can connect to the database and query the data. \n",
    "    * `Start time` and `End time` is required so that we can modify the query.\n",
    "\n",
    "* ***`Output`***: It will return the list.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_valid_refund_data(db, start_time, end_time):\n",
    "    \n",
    "    # create database cursor\n",
    "    cursor = db.cursor()\n",
    "    \n",
    "    print(\"Extracting valid refund data between {} and {}\".format(str(start_time),str(end_time)))\n",
    "    \n",
    "    # command to extract the data of last 30 minutes.\n",
    "    command = \"SELECT transaction_id FROM valid_refund WHERE ticket_raise_time BETWEEN '{}' AND '{}'\".format(start_time, end_time)\n",
    "    \n",
    "    # execute the command and return the results.\n",
    "    cursor.execute(command)\n",
    "    data = cursor.fetchall()\n",
    "    \n",
    "    # return the valid transaction IDs list\n",
    "    return pd.DataFrame.from_records(data, columns= ['ticket_raise_time'])['ticket_raise_time'].to_list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Define the Trasformation Function`\n",
    "\n",
    "---\n",
    "\n",
    "* ***`transform_users_data()`***: It will use the data from the `refund_details` table , `transactions` table & `valid_refunds` table of and do the following transformation using pandas.\n",
    "    * Merge the transaction data and the refunds data.\n",
    "    * If the transaction id matches, it is `valid refund request`.\n",
    "    * If any duplicate transaction id occurs in the list, remove it. There is a possibility that a person asks for refund multiple times for a single transaction.\n",
    "    * If the transaction id doesn't match, then it is a `invalid refund request`.\n",
    "    * Save the `valid refund request` and `invalid refund request` in a separate dataframe or list and return it. \n",
    "    \n",
    "<br>    \n",
    "\n",
    "* ***`Paramaters Required`***:\n",
    "    * `df_refund`: refund requests data dataframe of last 30 minutes. \n",
    "    * `df_transaction_id`: transaction data of last 48 hours\n",
    "    * `refund_issued_TID`: list of valid transaction ids in the last 48 hours.\n",
    "\n",
    "<br>\n",
    "\n",
    "* ***`Output`***: It will return two details `valid refund request` and `invalid refund request`.\n",
    "\n",
    "---\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_refund_data(df_refund, df_transactions, refund_issued_TID):\n",
    "    \n",
    "    print(\"Transforming Refund Data...\")\n",
    "    # merge the transactions data and the refund request\n",
    "    # refund request is for the last 30 minutes\n",
    "    # transactions data is for the last 48 hours\n",
    "    valid_transactions = df_transactions.merge(df_refund, how='left', on='transaction_id')\n",
    "    valid_transactions = valid_transactions[valid_transactions.ticket_id.isnull() == False]\n",
    "    \n",
    "    # keep only one row for each transaction id\n",
    "    valid_transactions = valid_transactions.groupby(['transaction_id']).first().reset_index()\n",
    "    \n",
    "    # if the transaction id is available in refund issued, then it is a duplicate request\n",
    "    # otherwise it is a final valid request\n",
    "    valid_transactions_final = valid_transactions[~valid_transactions.transaction_id.isin(refund_issued_TID)]\n",
    "    valid_transaction_duplicate = valid_transactions[valid_transactions.transaction_id.isin(refund_issued_TID)]\n",
    "    \n",
    "    \n",
    "    # select the columns for the final output.\n",
    "    valid_transactions_final = valid_transactions_final[['ticket_id', 'transaction_id', 'user_id', 'price', 'ticket_raise_time']]\n",
    "    \n",
    "    # convert the dataframe into a list of tuples as we have to do the multiple inserts in the table\n",
    "    valid_transactions_final = [tuple((row[0],row[1],row[2],int(row[3]), str(row[4]))) for row in valid_transactions_final.to_records(index=False)]\n",
    "    \n",
    "    # list of rejected refunds.\n",
    "    # create a new column refund reject reason\n",
    "    # if the transaction id doesn't match, mark it as `transaction_id_not_matched`.\n",
    "    # if the request is duplicate, mark it as `already_processed`\n",
    "    refund_rejected = df_refund.merge(df_transactions, how='left', on='transaction_id')\n",
    "    refund_rejected['refund_reject_reason'] = None\n",
    "    refund_rejected.loc[refund_rejected.product_id.isna() == True, 'refund_reject_reason'] = 'transaction_id_not_matched'\n",
    "    refund_rejected.loc[refund_rejected.transaction_id.isin(refund_issued_TID), 'refund_reject_reason'] = 'already_processed'\n",
    "    \n",
    "    # select the final columns and return the result.\n",
    "    refund_rejected = refund_rejected[refund_rejected.refund_reject_reason.isna() == False]\n",
    "    refund_rejected = refund_rejected[['ticket_id', 'user_name', 'refund_reject_reason']]\n",
    "    \n",
    "    \n",
    "    return valid_transactions_final, refund_rejected\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Define the Loading Function`\n",
    "\n",
    "---\n",
    "\n",
    "* ***`load_refund_summary()`***: It will use the results dataframe and list from the `transform_refund_data` and load it into the `valid_refund_table` and a CSV file.\n",
    "\n",
    "    \n",
    "<br>    \n",
    "\n",
    "* ***`Paramaters Required`***:\n",
    "    * `valid_refunds` final list of records to be added into valid refunds table. \n",
    "    * `rejected_requests` dataframe that needs to be converted into the rejected request list.\n",
    "    * `db` database connections\n",
    "\n",
    "<br>\n",
    "\n",
    "* ***`Output`***: It will not return anything.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_refund_data(valid_refunds, rejected_requests, db):\n",
    "    print(\"Loading Valid Refunds Table...\")\n",
    "    cursor = db.cursor()\n",
    "    # command to insert into into the valid_refund table\n",
    "    command = \"INSERT INTO valid_refund(ticket_id, transaction_id, user_id, price, ticket_raise_time) values(%s, %s, %s, %s, %s)\"\n",
    "    cursor.executemany(command, valid_refunds)\n",
    "    db.commit()\n",
    "    # rejected request to be converted into the CSV file and uploaded into the output_folder\n",
    "    file_name = \"rejected_request\" + str(pd.datetime.now()) + \".csv\"\n",
    "    rejected_requests.to_csv(\"output_folder/\" + file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "#### `Define the ETL Pipeline`\n",
    "\n",
    "<br>\n",
    "\n",
    "Now, we will define the pipeline function, we will take the database connection as the parameter and do the following steps.\n",
    "\n",
    " * Now, we will define the time for which we want to extract the data.\n",
    " * Extract the latest 30 minutes of users data.\n",
    " * Extract the latest 48 hours of transactions data.\n",
    " * Extract the lastest 48 hours of refund requests data.\n",
    " * Transform it to get the `valid refunds request` and `invalid refund requests`.\n",
    " * Load the data into the valid_refunds table and a CSV file.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_to_update_valid_refunds(db_object):\n",
    "    \n",
    "    current_time = datetime.datetime.now()\n",
    "    current_minus_30 = current_time - datetime.timedelta(minutes = 30)\n",
    "    current_minus_48 = current_time - datetime.timedelta(hours  = 48)\n",
    "    \n",
    "    print(\"========================================================================================\")\n",
    "    print(\"Starting ETL to update valid refunds data!!\")\n",
    "    \n",
    "    ## EXTRACTION\n",
    "    latest_refund_requests = extract_refund_data(db = db_object,\n",
    "                                                 start_time = current_minus_30,\n",
    "                                                 end_time = current_time)\n",
    "    \n",
    "    valid_transactions = extract_transaction_data(db = db_object,\n",
    "                                                  start_time = current_minus_48,\n",
    "                                                  end_time = current_time)\n",
    "   \n",
    "    refund_issued = extract_valid_refund_data(db = db_object,\n",
    "                                              start_time = current_minus_48,\n",
    "                                              end_time = current_time)\n",
    "    \n",
    "    ## TRANSFORMATION\n",
    "    valid_refunds, refunds_rejected = transform_refund_data(df_refund=latest_refund_requests,\n",
    "                                                            df_transactions=valid_transactions,\n",
    "                                                            refund_issued_TID= refund_issued)\n",
    "    \n",
    "    ## LOADING\n",
    "    load_refund_data(db = db_object,\n",
    "                     valid_refunds = valid_refunds,\n",
    "                     rejected_requests = refunds_rejected)\n",
    "    \n",
    "    print(\"Successfully loaded the data into valid refund table !!\")\n",
    "    print(\"========================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create the pipeline 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<center><h1> Schedule Pipelines </h1></center>\n",
    "\n",
    "---\n",
    "\n",
    "![](images/pipeline-4.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Every 80 seconds do pipeline_to_update_valid_refunds(db_object=<mysql.connector.connection_cext.CMySQLConnection object at 0x7f82a02c7290>) (last run: [never], next run: 2021-09-28 15:07:35)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Schedule Pipelines\n",
    "\n",
    "db_object = create_connection()\n",
    "products_data = pd.read_csv('dataset/product_table.csv')\n",
    "\n",
    "# pipeline 1: every 300 seconds\n",
    "schedule.every(30).seconds.do(pipeline_to_update_user_summary, db_object = db_object)\n",
    "# pipeline 2: every 600 seconds\n",
    "schedule.every(60).seconds.do(pipeline_to_update_transaction_summary, db_object = db_object, products_data = products_data)\n",
    "# pipeline 3: every 1800 seconds\n",
    "schedule.every(80).seconds.do(pipeline_to_update_valid_refunds, db_object = db_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================\n",
      "Starting ETL to update user summary!!\n",
      "Extracting signup results between 2021-09-28 15:01:46.553131 and 2021-09-28 15:06:46.553131\n",
      "Transforming User Data...\n",
      "Loading User Summary Table...\n",
      "Successfully loaded the data into user summary table !!\n",
      "========================================================================================\n",
      "========================================================================================\n",
      "Starting ETL to update transaction summary!!\n",
      "Extracting transactions between 2021-09-28 14:57:15.737396 and 2021-09-28 15:07:15.737396\n",
      "Transforming Transaction Data...\n",
      "Loading Transaction Summary Table...\n",
      "Successfully loaded the data into transaction summary table !!\n",
      "========================================================================================\n",
      "========================================================================================\n",
      "Starting ETL to update user summary!!\n",
      "Extracting signup results between 2021-09-28 15:02:16.894747 and 2021-09-28 15:07:16.894747\n",
      "Transforming User Data...\n",
      "Loading User Summary Table...\n",
      "Successfully loaded the data into user summary table !!\n",
      "========================================================================================\n",
      "========================================================================================\n",
      "Starting ETL to update valid refunds data!!\n",
      "Extracting refund between 2021-09-28 14:37:36.534991 and 2021-09-28 15:07:36.534991\n",
      "Extracting transactions between 2021-09-26 15:07:36.534991 and 2021-09-28 15:07:36.534991\n",
      "Extracting valid refund data between 2021-09-26 15:07:36.534991 and 2021-09-28 15:07:36.534991\n",
      "Transforming Refund Data...\n",
      "Loading Valid Refunds Table...\n",
      "Successfully loaded the data into valid refund table !!\n",
      "========================================================================================\n",
      "========================================================================================\n",
      "Starting ETL to update user summary!!\n",
      "Extracting signup results between 2021-09-28 15:02:47.837710 and 2021-09-28 15:07:47.837710\n",
      "Transforming User Data...\n",
      "Loading User Summary Table...\n",
      "Successfully loaded the data into user summary table !!\n",
      "========================================================================================\n",
      "========================================================================================\n",
      "Starting ETL to update transaction summary!!\n",
      "Extracting transactions between 2021-09-28 14:58:16.044538 and 2021-09-28 15:08:16.044538\n",
      "Transforming Transaction Data...\n",
      "Loading Transaction Summary Table...\n",
      "Successfully loaded the data into transaction summary table !!\n",
      "========================================================================================\n",
      "========================================================================================\n",
      "Starting ETL to update user summary!!\n",
      "Extracting signup results between 2021-09-28 15:03:18.264474 and 2021-09-28 15:08:18.264474\n",
      "Transforming User Data...\n",
      "Loading User Summary Table...\n",
      "Successfully loaded the data into user summary table !!\n",
      "========================================================================================\n",
      "========================================================================================\n",
      "Starting ETL to update user summary!!\n",
      "Extracting signup results between 2021-09-28 15:03:48.460916 and 2021-09-28 15:08:48.460916\n",
      "Transforming User Data...\n",
      "Loading User Summary Table...\n",
      "Successfully loaded the data into user summary table !!\n",
      "========================================================================================\n",
      "========================================================================================\n",
      "Starting ETL to update valid refunds data!!\n",
      "Extracting refund between 2021-09-28 14:38:57.598400 and 2021-09-28 15:08:57.598400\n",
      "Extracting transactions between 2021-09-26 15:08:57.598400 and 2021-09-28 15:08:57.598400\n",
      "Extracting valid refund data between 2021-09-26 15:08:57.598400 and 2021-09-28 15:08:57.598400\n",
      "Transforming Refund Data...\n",
      "Loading Valid Refunds Table...\n",
      "Successfully loaded the data into valid refund table !!\n",
      "========================================================================================\n",
      "========================================================================================\n",
      "Starting ETL to update transaction summary!!\n",
      "Extracting transactions between 2021-09-28 14:59:16.867492 and 2021-09-28 15:09:16.867492\n",
      "Transforming Transaction Data...\n",
      "Loading Transaction Summary Table...\n",
      "Successfully loaded the data into transaction summary table !!\n",
      "========================================================================================\n",
      "========================================================================================\n",
      "Starting ETL to update user summary!!\n",
      "Extracting signup results between 2021-09-28 15:04:19.025614 and 2021-09-28 15:09:19.025614\n",
      "Transforming User Data...\n",
      "Loading User Summary Table...\n",
      "Successfully loaded the data into user summary table !!\n",
      "========================================================================================\n",
      "========================================================================================\n",
      "Starting ETL to update user summary!!\n",
      "Extracting signup results between 2021-09-28 15:04:49.241774 and 2021-09-28 15:09:49.241774\n",
      "Transforming User Data...\n",
      "Loading User Summary Table...\n",
      "Successfully loaded the data into user summary table !!\n",
      "========================================================================================\n",
      "========================================================================================\n",
      "Starting ETL to update transaction summary!!\n",
      "Extracting transactions between 2021-09-28 15:00:17.418384 and 2021-09-28 15:10:17.418384\n",
      "Transforming Transaction Data...\n",
      "Loading Transaction Summary Table...\n",
      "Successfully loaded the data into transaction summary table !!\n",
      "========================================================================================\n",
      "========================================================================================\n",
      "Starting ETL to update valid refunds data!!\n",
      "Extracting refund between 2021-09-28 14:40:18.510757 and 2021-09-28 15:10:18.510757\n",
      "Extracting transactions between 2021-09-26 15:10:18.510757 and 2021-09-28 15:10:18.510757\n",
      "Extracting valid refund data between 2021-09-26 15:10:18.510757 and 2021-09-28 15:10:18.510757\n",
      "Transforming Refund Data...\n",
      "Loading Valid Refunds Table...\n",
      "Successfully loaded the data into valid refund table !!\n",
      "========================================================================================\n",
      "========================================================================================\n",
      "Starting ETL to update user summary!!\n",
      "Extracting signup results between 2021-09-28 15:05:19.646367 and 2021-09-28 15:10:19.646367\n",
      "Transforming User Data...\n",
      "Loading User Summary Table...\n",
      "Successfully loaded the data into user summary table !!\n",
      "========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# start the schedular\n",
    "while True:\n",
    "        schedule.run_pending() \n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
